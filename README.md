# HPC-Project

GitHub repository for HPC Project @ Unitn by @me and @Sano. <a href"https://www.canva.com/design/DAG-bSRRKxU/EV06j1UHVTkWS7GRp_nIPQ/view?utm_content=DAG-bSRRKxU&utm_campaign=designshare&utm_medium=link2&utm_source=uniquelinks&utlId=ha9df6f653d">Here</a> the presentation of the project.

Academic year 2025-2026.

In this repository, we provide two parallel algorithms for computing the double integral over a triangular region.
Both exploit a Romberg-like technique.

The <a href="group_implementation\parallel_impl\parallel_implementation.c">first</a> algorithm is a hybrid program (MPI + OpenMP) that uses *MPI_Scatterv*, *MPI_Bcast* and *MPI_Reduce* to implement a points distribution logic
and parallelize to workload among all available processes.

The <a href="paper_implementation\implementation\romberg_2d_buffered.c">second</a> algorithm is a MPI program where points are generated by a master process and sent in buffer to others workers processes that will elaborate them. This is a possible C implementation for the pseudo-code in the <a href="related_work\The_romberg-like_parallel_numerical_integration_on_a_cluster_system.pdf">related work</a> where the problem is well explained. 

# Setup

Hardware setup (Unitn CLUSTER HPC2):
- 422,7 TFLOP of theroetical peak performance CPU
- 7674 cores
- 142 calculation nodes
- 10 Gb/s network (some nodes with Infiniband up to 40 Gb/s and some with Omin Path up to 56 Gb/s)

Software setup:
- MPICH version 3.2.
- Python version 3.10.14.
- GCC version 4.8.5.
- Operating system CentOS Linux 7.9
- Scheduler openPBS 19.1

# Code execution - OUR algorithm

To run the code, relies on the following bash files:
- <a href="group_implementation/serial_impl/serial_implementation.sh">serial_implementation.sh</a>: to run the SERIAL version
- <a href="group_implementation/parallel_impl/parallel_implementation.sh">parallel_implementation.sh</a>: to run the HYBRID PARALLEL version (MPI + OpenMP)
    - Specify proper MPI processes to use and threads to use and ask for proper number of chunks and cores per chunk
- <a href="group_implementation/benchmarking/benchmarking.sh">benchmarking.sh</a>: to run the BENCHMARKING

In case of cluster execution, ask for proper number of cores (1 core per every thread).

In any case, export to all MPI processes two environment variables:
- ITER: set the complexity of the integrand function (represented by the number of 'dummy' iterations performed)
- OMP_NUM_THREADS: set the number of threads to generate per each MPI process

Any executable takes 6 double command line arguments: Ax Ay Bx By Cx Cy, specifically the x and y coordinates for the vertices of the cluster.

To run any of the programs with a different integrand function, change the function *double f(double x, double y)* in the source code and compile again, following the instructions in the corresponding bash file.

# Code execution - Master-Worker algorithm

To run the code, relies on the following bash files:
- <a href="paper_implementation/implementation/run_romberg_2D.sh">run_romberg_2D.sh</a>: to run the SERIAL version and the PARALLEL version (MPI only)
    - Specify proper MPI processes to use (1 for serial version) and ask for equal number of cores
    - To change the integrand function, the computational complexity or the buffer size, change the source code *romberg_2d_buffered.c* and compile again, following the instructions in the bash file.
    - It is possible to change vertices of the triangles by passing 6 double command line arguments, as explained above.
- <a href="paper_implementation/benchmarking/bench_romberg.sh">bench_romberg.sh</a>: to run the BENCHMARKING
    - To change the integrand function or the vertices of the triangle, change the source code *benchmarking.c* and compile again, following the instructions in the bash file.
    - Computational complexity of the function and buffer sizes are passed by command line argument (first argument is buffer size and second is computational complexity).

In case of cluster execution, ask for proper number of cores (1 core per every MPI process).

# Where can I see the performances of your algorithm?
- For our algorithm, all results of benchmarking are available <a href="/group_implementation/benchmarking/results/performances.md">here</a>.
- For the implementation of the Master-Worker algorithm, all results of benchmarking are available <a href="/paper_implementation/benchmarking/results/benchmark_results.csv">here</a>.

# Project Structure

```
HPC-Project/
├── Report_HPC_Volani_Hamburdzamyan.pdf               # Official report of the repository
|
├── related_work/                                     # Directory with info about the related work that inspired our project
|
├── paper_implementation/                             # Directory for the MASTER-WORKER ALGORITHM
│   ├── implementation/                               # Directory with PARALLEL source code, executable program and bash script for cluster runs 
│   ├── benchmarking/                                 # Directory with source code of benchmarking and RESULTS
│   └── data_dependency_analysis/                     # Directory with data dependency analysis
│   
└── group_implementation/                             # Directory for OUR ALGORITHM
    ├── parallel_impl/                                # Directory with PARALLEL source code, executable program and bash script for cluster runs
    ├── serial_impl/                                  # Directory with SERIAL source code, executable program and bash script for cluster runs 
    ├── benchmarking/                                 # Directory with source code of benchmarking and RESULTS
    └── data_dependency_analysis/                     # Directory with data dependency analysis
```
